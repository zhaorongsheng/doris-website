"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["928695"],{184189:function(e,n,t){t.r(n),t.d(n,{default:()=>h,frontMatter:()=>s,metadata:()=>i,assets:()=>c,toc:()=>o,contentTitle:()=>l});var i=JSON.parse('{"id":"lakehouse/datalake-building/iceberg-build","title":"Iceberg","description":"\x3c!--","source":"@site/versioned_docs/version-3.0/lakehouse/datalake-building/iceberg-build.md","sourceDirName":"lakehouse/datalake-building","slug":"/lakehouse/datalake-building/iceberg-build","permalink":"/docs/3.0/lakehouse/datalake-building/iceberg-build","draft":false,"unlisted":false,"tags":[],"version":"3.0","frontMatter":{"title":"Iceberg","language":"en"},"sidebar":"docs","previous":{"title":"Hive","permalink":"/docs/3.0/lakehouse/datalake-building/hive-build"},"next":{"title":"JDBC Catalog","permalink":"/docs/3.0/lakehouse/database/jdbc"}}'),r=t("785893"),a=t("250065");let s={title:"Iceberg",language:"en"},l=void 0,c={},o=[{value:"Metadata Creation and Deletion",id:"metadata-creation-and-deletion",level:2},{value:"Catalog",id:"catalog",level:3},{value:"Database",id:"database",level:3},{value:"Table",id:"table",level:3},{value:"Data Operations",id:"data-operations",level:2},{value:"INSERT",id:"insert",level:3},{value:"INSERT OVERWRITE",id:"insert-overwrite",level:3},{value:"CTAS(CREATE TABLE AS SELECT)",id:"ctascreate-table-as-select",level:3},{value:"Abnormal Data and Data Transformation",id:"abnormal-data-and-data-transformation",level:2},{value:"HDFS File Operations",id:"hdfs-file-operations",level:3},{value:"Object Storage File Operations",id:"object-storage-file-operations",level:3},{value:"Related Parameters",id:"related-parameters",level:2},{value:"FE",id:"fe",level:3},{value:"BE",id:"be",level:3}];function d(e){let n={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Since version 2.1.6, Apache Doris supports DDL and DML operations on Iceberg. Users can directly create library tables in Iceberg through Apache Doris and write data to Iceberg tables. With this feature, users can perform full data querying and writing operations on Iceberg through Apache Doris, further simplifying the lakehouse architecture for users."}),"\n",(0,r.jsx)(n.p,{children:"This article introduces the Iceberg operations, syntax, and usage notes supported in Apache Doris."}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["Before using, please set:\n",(0,r.jsx)(n.br,{}),"\nset global enable_nereids_planner = true;\n",(0,r.jsx)(n.br,{}),"\nset global enable_fallback_to_original_planner = false;"]})}),"\n",(0,r.jsx)(n.h2,{id:"metadata-creation-and-deletion",children:"Metadata Creation and Deletion"}),"\n",(0,r.jsx)(n.h3,{id:"catalog",children:"Catalog"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Creation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'CREATE CATALOG [IF NOT EXISTS] iceberg PROPERTIES (\n    "type" = "iceberg",\n    "iceberg.catalog.type" = "hms",\n    "hive.metastore.uris" = "thrift://172.21.16.47:7004",\n    "warehouse" = "hdfs://172.21.16.47:4007/user/hive/warehouse/",\n    "hadoop.username" = "hadoop",\n    "fs.defaultFS" = "hdfs://172.21.16.47:4007"\n);\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The above mainly demonstrates how to create an HMS Iceberg Catalog in Apache Doris. Apache Doris currently supports multiple types of Iceberg Catalogs. For more configurations, please refer to ",(0,r.jsx)(n.a,{href:"/docs/3.0/lakehouse/datalake-analytics/iceberg",children:"Iceberg Catalog"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Note that if you need to create Iceberg tables or write data through Hive Catalog in Apache Doris, you need to explicitly add the ",(0,r.jsx)(n.code,{children:"fs.defaultFS"})," property and ",(0,r.jsx)(n.code,{children:"warehouse"})," property in the Catalog attributes. If the Catalog is created only for querying, these two parameters can be omitted."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Deletion"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"DROP CATALOG [IF EXISTS] iceberg;\n"})}),"\n",(0,r.jsx)(n.p,{children:"Deleting the Catalog does not remove any library table information in Iceberg. It simply removes the mapping of this Iceberg Catalog in Apache Doris."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"database",children:"Database"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Creation"}),"\n",(0,r.jsxs)(n.p,{children:["You can switch to the corresponding Catalog and execute the ",(0,r.jsx)(n.code,{children:"CREATE DATABASE"})," statement:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"SWITCH iceberg;\nCREATE DATABASE [IF NOT EXISTS] iceberg_db;\n"})}),"\n",(0,r.jsx)(n.p,{children:"You can also create using fully qualified names or specify a location, such as:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"CREATE DATABASE [IF NOT EXISTS] iceberg.iceberg_db;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Afterwards, you can use the ",(0,r.jsx)(n.code,{children:"SHOW CREATE DATABASE"})," command to view the relevant information of the Database:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"mysql> SHOW CREATE DATABASE iceberg_db;\n+------------+------------------------------+\n| Database   | Create Database              |\n+------------+------------------------------+\n| iceberg_db | CREATE DATABASE `iceberg_db` |\n+------------+------------------------------+\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Delete"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"DROP DATABASE [IF EXISTS] iceberg.iceberg_db;\n"})}),"\n",(0,r.jsx)(n.admonition,{title:"Note",type:"caution",children:(0,r.jsx)(n.p,{children:"For Iceberg Database, you must first delete all tables under this Database before deleting the Database, otherwise an error will occur. This operation will also synchronously delete the corresponding Database in Iceberg."})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"table",children:"Table"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Create"}),"\n",(0,r.jsx)(n.p,{children:"Apache Doris supports creating partitioned or non-partitioned tables in Iceberg."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"-- Create unpartitioned iceberg table\nCREATE TABLE unpartitioned_table (\n  `col1` BOOLEAN COMMENT 'col1',\n  `col2` INT COMMENT 'col2',\n  `col3` BIGINT COMMENT 'col3',\n  `col4` FLOAT COMMENT 'col4',\n  `col5` DOUBLE COMMENT 'col5',\n  `col6` DECIMAL(9,4) COMMENT 'col6',\n  `col7` STRING COMMENT 'col7',\n  `col8` DATE COMMENT 'col8',\n  `col9` DATETIME COMMENT 'col9'\n)  ENGINE=iceberg\nPROPERTIES (\n  'write-format'='parquet'\n);\n\n-- Create partitioned iceberg table\n-- The partition columns must be in table's column definition list\nCREATE TABLE partition_table (\n  `ts` DATETIME COMMENT 'ts',\n  `col1` BOOLEAN COMMENT 'col1',\n  `col2` INT COMMENT 'col2',\n  `col3` BIGINT COMMENT 'col3',\n  `col4` FLOAT COMMENT 'col4',\n  `col5` DOUBLE COMMENT 'col5',\n  `col6` DECIMAL(9,4) COMMENT 'col6',\n  `col7` STRING COMMENT 'col7',\n  `col8` DATE COMMENT 'col8',\n  `col9` DATETIME COMMENT 'col9',\n  `pt1` STRING COMMENT 'pt1',\n  `pt2` STRING COMMENT 'pt2'\n)  ENGINE=iceberg\nPARTITION BY LIST (DAY(ts), pt1, pt2) ()\nPROPERTIES (\n  'write-format'='orc',\n  'compression-codec'='zlib'\n);\n"})}),"\n",(0,r.jsxs)(n.p,{children:["After creation, you can use the ",(0,r.jsx)(n.code,{children:"SHOW CREATE TABLE"})," command to view the table creation statement in Iceberg. For partitioned tables and partition functions, refer to the following section on ",(0,r.jsx)(n.strong,{children:"Partition"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Column Types"}),"\n",(0,r.jsx)(n.p,{children:"The column types used to create Iceberg tables in Apache Doris correspond to the column types in Iceberg as follows:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Apache Doris"}),(0,r.jsx)(n.th,{children:"Iceberg"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"BOOLEAN"}),(0,r.jsx)(n.td,{children:"BOOLEAN"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"INT"}),(0,r.jsx)(n.td,{children:"INT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"BIGINT"}),(0,r.jsx)(n.td,{children:"BIGINT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FLOAT"}),(0,r.jsx)(n.td,{children:"FLOAT"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DOUBLE"}),(0,r.jsx)(n.td,{children:"DOUBLE"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DECIMAL"}),(0,r.jsx)(n.td,{children:"DECIMAL"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"STRING"}),(0,r.jsx)(n.td,{children:"STRING"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DATE"}),(0,r.jsx)(n.td,{children:"DATE"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DATETIME"}),(0,r.jsx)(n.td,{children:"TIMESTAMP"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"ARRAY"}),(0,r.jsx)(n.td,{children:"ARRAY"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"MAP"}),(0,r.jsx)(n.td,{children:"MAP"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"STRUCT"}),(0,r.jsx)(n.td,{children:"STRUCT"})]})]})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Note: Only these data types are currently supported; other data types will result in an error."}),"\n",(0,r.jsx)(n.li,{children:"Column types can only be the default Nullable for now; NOT NULL is not supported."}),"\n",(0,r.jsxs)(n.li,{children:["After inserting data, if the types are not compatible, such as inserting ",(0,r.jsx)(n.code,{children:"'abc'"})," into a numeric type, it will be converted to a null value before insertion."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Drop"}),"\n",(0,r.jsxs)(n.p,{children:["You can delete an Iceberg table using the ",(0,r.jsx)(n.code,{children:"DROP TABLE"})," statement. Currently, deleting a table will also delete the data, including partition data."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Partition"}),"\n",(0,r.jsx)(n.p,{children:"The partition types in Iceberg correspond to List partitions in Apache Doris. Therefore, when creating an Iceberg partitioned table in Apache Doris, you need to use the List partition creation statement, but you do not need to explicitly enumerate each partition. When writing data, Apache Doris will automatically create the corresponding Iceberg partition based on the data values."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Supports creating single-column or multi-column partitioned tables."}),"\n",(0,r.jsxs)(n.li,{children:["Supports partition transformation functions to support Iceberg implicit partitioning and partition evolution features. Specific Iceberg partition transformation functions can be found at ",(0,r.jsx)(n.a,{href:"https://iceberg.apache.org/spec/#partition-transforms",children:"Iceberg partition transforms"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"year(ts)"})," or ",(0,r.jsx)(n.code,{children:"years(ts)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"month(ts)"})," or ",(0,r.jsx)(n.code,{children:"months(ts)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"day(ts)"})," or ",(0,r.jsx)(n.code,{children:"days(ts)"})," or ",(0,r.jsx)(n.code,{children:"date(ts)"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"hour(ts)"})," or ",(0,r.jsx)(n.code,{children:"hours(ts)"})," or ",(0,r.jsx)(n.code,{children:"date_hour(ts)"})]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"bucket(N, col)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"truncate(L, col)"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"File Formats"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Parquet (default)"}),"\n",(0,r.jsx)(n.li,{children:"ORC"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Compression Formats"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Parquet: snappy, zstd (default), plain. (plain means no compression)"}),"\n",(0,r.jsx)(n.li,{children:"ORC: snappy, zlib (default), zstd, plain. (plain means no compression)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Storage Medium"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"HDFS"}),"\n",(0,r.jsx)(n.li,{children:"Object storage"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"data-operations",children:"Data Operations"}),"\n",(0,r.jsx)(n.p,{children:"You can use the INSERT statement to write data into an Iceberg table."}),"\n",(0,r.jsx)(n.p,{children:"Supports writing to Iceberg tables created by Apache Doris or to existing tables in Iceberg that support the format."}),"\n",(0,r.jsx)(n.p,{children:"For partitioned tables, data will be automatically written to the corresponding partition or a new partition will be created based on the data."}),"\n",(0,r.jsx)(n.p,{children:"Currently, specifying partition write is not supported."}),"\n",(0,r.jsx)(n.h3,{id:"insert",children:"INSERT"}),"\n",(0,r.jsx)(n.p,{children:"The INSERT operation writes data to the target table in an append manner."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'INSERT INTO iceberg_tbl values (val1, val2, val3, val4);\nINSERT INTO iceberg.iceberg_db.iceberg_tbl SELECT col1, col2 FROM internal.db1.tbl1;\n\nINSERT INTO iceberg_tbl(col1, col2) values (val1, val2);\nINSERT INTO iceberg_tbl(col1, col2, partition_col1, partition_col2) values (1, 2, "beijing", "2023-12-12");\n'})}),"\n",(0,r.jsx)(n.h3,{id:"insert-overwrite",children:"INSERT OVERWRITE"}),"\n",(0,r.jsx)(n.p,{children:"INSERT OVERWRITE completely replaces the existing data in the original table with new data."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"INSERT OVERWRITE TABLE VALUES(val1, val2, val3, val4)\nINSERT OVERWRITE TABLE iceberg.iceberg_db.iceberg_tbl(col1, col2) SELECT col1, col2 FROM internal.db1.tbl1;\n"})}),"\n",(0,r.jsx)(n.h3,{id:"ctascreate-table-as-select",children:"CTAS(CREATE TABLE AS SELECT)"}),"\n",(0,r.jsxs)(n.p,{children:["You can create an Iceberg table and write data using the ",(0,r.jsx)(n.code,{children:"CTAS"})," statement:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"CREATE TABLE iceberg_ctas ENGINE=iceberg AS SELECT * FROM other_table;\n"})}),"\n",(0,r.jsx)(n.p,{children:"CTAS supports specifying file formats, partitioning methods, and other information, such as:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"CREATE TABLE iceberg_ctas ENGINE=iceberg\nPARTITION BY LIST (pt1, pt2) ()\nAS SELECT col1,pt1,pt2 FROM part_ctas_src WHERE col1>0;\n    \nCREATE TABLE iceberg.iceberg_db.iceberg_ctas (col1,col2,pt1) ENGINE=iceberg\nPARTITION BY LIST (pt1) ()\nPROPERTIES (\n    'write-format'='parquet',\n    'compression-codec'='zstd'\n)\nAS SELECT col1,pt1 as col2,pt2 as pt1 FROM test_ctas.part_ctas_src WHERE col1>0;\n"})}),"\n",(0,r.jsx)(n.h2,{id:"abnormal-data-and-data-transformation",children:"Abnormal Data and Data Transformation"}),"\n",(0,r.jsx)(n.p,{children:"TODO"}),"\n",(0,r.jsx)(n.h3,{id:"hdfs-file-operations",children:"HDFS File Operations"}),"\n",(0,r.jsx)(n.p,{children:"Data from Iceberg tables on HDFS is written to the final directory, and Iceberg metadata is managed by submitting it."}),"\n",(0,r.jsxs)(n.p,{children:["The data file naming format is: ",(0,r.jsx)(n.code,{children:"<query-id>_<uuid>-<index>.<compress-type>.<file-type>"})]}),"\n",(0,r.jsx)(n.h3,{id:"object-storage-file-operations",children:"Object Storage File Operations"}),"\n",(0,r.jsx)(n.p,{children:"TODO"}),"\n",(0,r.jsx)(n.h2,{id:"related-parameters",children:"Related Parameters"}),"\n",(0,r.jsx)(n.h3,{id:"fe",children:"FE"}),"\n",(0,r.jsx)(n.p,{children:"TODO"}),"\n",(0,r.jsx)(n.h3,{id:"be",children:"BE"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter Name"}),(0,r.jsx)(n.th,{children:"Default Value"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"iceberg_sink_max_file_size"})}),(0,r.jsx)(n.td,{children:"Maximum size of data files. When the amount of data written exceeds this size, the current file will be closed, and a new file will be created to continue writing."}),(0,r.jsx)(n.td,{children:"1GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"table_sink_partition_write_max_partition_nums_per_writer"})}),(0,r.jsx)(n.td,{children:"Maximum number of partitions written per Instance on a BE node."}),(0,r.jsx)(n.td,{children:"128"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"table_sink_non_partition_write_scaling_data_processed_threshold"})}),(0,r.jsxs)(n.td,{children:["Threshold for the amount of data processed to trigger scaling-write for non-partitioned tables. For every increase of ",(0,r.jsx)(n.code,{children:"table_sink_non_partition_write_scaling_data_processed_threshold"})," data, it will be sent to a new writer (instance) for writing. The scaling-write mechanism is mainly used to write data using a different number of writer (instance) based on the data volume, increasing the number of writer (instance) with the increase in data volume to improve concurrent write throughput. It also saves resources when the data volume is low and minimizes the number of generated files as much as possible."]}),(0,r.jsx)(n.td,{children:"25MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"table_sink_partition_write_min_data_processed_rebalance_threshold"})}),(0,r.jsxs)(n.td,{children:["Minimum data amount threshold to trigger rebalancing for partitioned tables. If ",(0,r.jsx)(n.code,{children:"accumulated data amount"})," - ",(0,r.jsx)(n.code,{children:"data amount since the last rebalance or initial accumulation"})," >= ",(0,r.jsx)(n.code,{children:"table_sink_partition_write_min_data_processed_rebalance_threshold"}),", the rebalancing mechanism will be triggered. If a significant difference in final file sizes is found, you can reduce this threshold to increase balance. Of course, a too small threshold will increase the cost of rebalancing and may affect performance."]}),(0,r.jsx)(n.td,{children:"25MB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"table_sink_partition_write_min_partition_data_processed_rebalance_threshold"})}),(0,r.jsxs)(n.td,{children:["Minimum partition data amount threshold when triggering rebalancing for partitioned tables. If ",(0,r.jsx)(n.code,{children:"current partition data amount"})," >= ",(0,r.jsx)(n.code,{children:"threshold"})," * ",(0,r.jsx)(n.code,{children:"current partition allocated task number"}),", the partition will be rebalanced. If a significant difference in final file sizes is found, you can reduce this threshold to increase balance. Of course, a too small threshold will increase the cost of rebalancing and may affect performance."]}),(0,r.jsx)(n.td,{children:"15MB"})]})]})]})]})}function h(e={}){let{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},250065:function(e,n,t){t.d(n,{Z:function(){return l},a:function(){return s}});var i=t(667294);let r={},a=i.createContext(r);function s(e){let n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);