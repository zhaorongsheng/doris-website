"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([["609716"],{121555:function(e,n,s){s.r(n),s.d(n,{default:()=>h,frontMatter:()=>a,metadata:()=>t,assets:()=>c,toc:()=>d,contentTitle:()=>r});var t=JSON.parse('{"id":"lakehouse/file","title":"Querying Files on S3/HDFS","description":"\x3c!--","source":"@site/versioned_docs/version-2.0/lakehouse/file.md","sourceDirName":"lakehouse","slug":"/lakehouse/file","permalink":"/docs/2.0/lakehouse/file","draft":false,"unlisted":false,"tags":[],"version":"2.0","frontMatter":{"title":"Querying Files on S3/HDFS","language":"en"},"sidebar":"docs","previous":{"title":"MaxCompute","permalink":"/docs/2.0/lakehouse/database/max-compute"},"next":{"title":"File Caches","permalink":"/docs/2.0/lakehouse/filecache"}}'),i=s("785893"),l=s("250065");let a={title:"Querying Files on S3/HDFS",language:"en"},r=void 0,c={},d=[{value:"Usage",id:"usage",level:2},{value:"Automatic Column Type Inference",id:"automatic-column-type-inference",level:3},{value:"Query and Analysis",id:"query-and-analysis",level:3},{value:"Data Ingestion",id:"data-ingestion",level:3},{value:"Note",id:"note",level:3}];function o(e){let n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"With the Table Value Function feature, Doris is able to query files in object storage or HDFS as simply as querying Tables. In addition, it supports automatic column type inference."}),"\n",(0,i.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,i.jsx)(n.p,{children:"For more usage details, please see the documentation:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/docs/2.0/sql-manual/sql-functions/table-valued-functions/s3",children:"S3"}),": supports file analysis on object storage compatible with S3"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/docs/2.0/sql-manual/sql-functions/table-valued-functions/hdfs",children:"HDFS"}),": supports file analysis on HDFS"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/docs/2.0/sql-manual/sql-functions/table-valued-functions/local",children:"LOCAL"}),": supports file analysis on local file system"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The followings illustrate how file analysis is conducted with the example of S3 Table Value Function."}),"\n",(0,i.jsx)(n.h3,{id:"automatic-column-type-inference",children:"Automatic Column Type Inference"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'> DESC FUNCTION s3 (\n    "URI" = "http://127.0.0.1:9312/test2/test.snappy.parquet",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "parquet",\n    "use_path_style"="true"\n);\n+---------------+--------------+------+-------+---------+-------+\n| Field         | Type         | Null | Key   | Default | Extra |\n+---------------+--------------+------+-------+---------+-------+\n| p_partkey     | INT          | Yes  | false | NULL    | NONE  |\n| p_name        | TEXT         | Yes  | false | NULL    | NONE  |\n| p_mfgr        | TEXT         | Yes  | false | NULL    | NONE  |\n| p_brand       | TEXT         | Yes  | false | NULL    | NONE  |\n| p_type        | TEXT         | Yes  | false | NULL    | NONE  |\n| p_size        | INT          | Yes  | false | NULL    | NONE  |\n| p_container   | TEXT         | Yes  | false | NULL    | NONE  |\n| p_retailprice | DECIMAL(9,0) | Yes  | false | NULL    | NONE  |\n| p_comment     | TEXT         | Yes  | false | NULL    | NONE  |\n+---------------+--------------+------+-------+---------+-------+\n'})}),"\n",(0,i.jsx)(n.p,{children:"An S3 Table Value Function is defined as follows:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'s3(\n    "URI" = "http://127.0.0.1:9312/test2/test.snappy.parquet",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "Format" = "parquet",\n    "use_path_style"="true")\n'})}),"\n",(0,i.jsx)(n.p,{children:"It specifies the file path, connection, and authentication."}),"\n",(0,i.jsxs)(n.p,{children:["After defining, you can view the schema of this file using the ",(0,i.jsx)(n.code,{children:"DESC FUNCTION"})," statement."]}),"\n",(0,i.jsx)(n.p,{children:"As can be seen, Doris is able to automatically infer column types based on the metadata of the Parquet file."}),"\n",(0,i.jsx)(n.p,{children:"Besides Parquet, Doris supports analysis and auto column type inference of ORC, CSV, and Json files."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"CSV Schema"})}),"\n",(0,i.jsxs)(n.p,{children:["By default, for CSV format files, all columns are of type String. Column names and column types can be specified individually via the ",(0,i.jsx)(n.code,{children:"csv_schema"})," attribute. Doris will use the specified column type for file reading. The format is as follows:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"name1:type1;name2:type2;..."})}),"\n",(0,i.jsx)(n.p,{children:"For columns with mismatched formats (such as string in the file and int defined by the user), or missing columns (such as 4 columns in the file and 5 columns defined by the user), these columns will return null."}),"\n",(0,i.jsx)(n.p,{children:"Currently supported column types are:"}),"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"name"}),(0,i.jsx)(n.th,{children:"mapping type"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"tinyint"}),(0,i.jsx)(n.td,{children:"tinyint"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"smallint"}),(0,i.jsx)(n.td,{children:"smallint"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"int"}),(0,i.jsx)(n.td,{children:"int"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"bigint"}),(0,i.jsx)(n.td,{children:"bigint"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"largeint"}),(0,i.jsx)(n.td,{children:"largeint"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"float"}),(0,i.jsx)(n.td,{children:"float"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"double"}),(0,i.jsx)(n.td,{children:"double"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"decimal(p,s)"}),(0,i.jsx)(n.td,{children:"decimalv3(p,s)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"date"}),(0,i.jsx)(n.td,{children:"datev2"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"datetime"}),(0,i.jsx)(n.td,{children:"datetimev2"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"char"}),(0,i.jsx)(n.td,{children:"string"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"varchar"}),(0,i.jsx)(n.td,{children:"string"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"string"}),(0,i.jsx)(n.td,{children:"string"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"boolean"}),(0,i.jsx)(n.td,{children:"boolean"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"Example:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'s3 (\n    "uri" = "https://bucket1/inventory.dat",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "csv",\n    "column_separator" = "|",\n    "csv_schema" = "k1:int;k2:int;k3:int;k4:decimal(38,10)",\n    "use_path_style"="true"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"query-and-analysis",children:"Query and Analysis"}),"\n",(0,i.jsx)(n.p,{children:"You can conduct queries and analysis on this Parquet file using any SQL statements:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'SELECT * FROM s3(\n    "uri" = "http://127.0.0.1:9312/test2/test.snappy.parquet",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "parquet",\n    "use_path_style"="true")\nLIMIT 5;\n+-----------+------------------------------------------+----------------+----------+-------------------------+--------+-------------+---------------+---------------------+\n| p_partkey | p_name                                   | p_mfgr         | p_brand  | p_type                  | p_size | p_container | p_retailprice | p_comment           |\n+-----------+------------------------------------------+----------------+----------+-------------------------+--------+-------------+---------------+---------------------+\n|         1 | goldenrod lavender spring chocolate lace | Manufacturer#1 | Brand#13 | PROMO BURNISHED COPPER  |      7 | JUMBO PKG   |           901 | ly. slyly ironi     |\n|         2 | blush thistle blue yellow saddle         | Manufacturer#1 | Brand#13 | LARGE BRUSHED BRASS     |      1 | LG CASE     |           902 | lar accounts amo    |\n|         3 | spring green yellow purple cornsilk      | Manufacturer#4 | Brand#42 | STANDARD POLISHED BRASS |     21 | WRAP CASE   |           903 | egular deposits hag |\n|         4 | cornflower chocolate smoke green pink    | Manufacturer#3 | Brand#34 | SMALL PLATED BRASS      |     14 | MED DRUM    |           904 | p furiously r       |\n|         5 | forest brown coral puff cream            | Manufacturer#3 | Brand#32 | STANDARD POLISHED TIN   |     15 | SM PKG      |           905 |  wake carefully     |\n+-----------+------------------------------------------+----------------+----------+-------------------------+--------+-------------+---------------+---------------------+\n'})}),"\n",(0,i.jsx)(n.p,{children:"You can put the Table Value Function anywhere that you used to put Table in the SQL, such as in the WITH or FROM clause in CTE. In this way, you can treat the file as a normal table and conduct analysis conveniently."}),"\n",(0,i.jsxs)(n.p,{children:["You can also create a logic view by using ",(0,i.jsx)(n.code,{children:"CREATE VIEW"})," statement for a Table Value Function. So that you can query this view, grant priv on this view or allow other user to access this Table Value Function."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'CREATE VIEW v1 AS \nSELECT * FROM s3(\n    "uri" = "http://127.0.0.1:9312/test2/test.snappy.parquet",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "parquet",\n    "use_path_style"="true");\n\nDESC v1;\n\nSELECT * FROM v1;\n\nGRANT SELECT_PRIV ON db1.v1 TO user1;\n'})}),"\n",(0,i.jsx)(n.h3,{id:"data-ingestion",children:"Data Ingestion"}),"\n",(0,i.jsxs)(n.p,{children:["Users can ingest files into Doris tables via  ",(0,i.jsx)(n.code,{children:"INSERT INTO SELECT"}),"  for faster file analysis:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sql",children:'// 1. Create Doris internal table\nCREATE TABLE IF NOT EXISTS test_table\n(\n    id int,\n    name varchar(50),\n    age int\n)\nDISTRIBUTED BY HASH(id) BUCKETS 4\nPROPERTIES("replication_num" = "1");\n\n// 2. Insert data using S3 Table Value Function\nINSERT INTO test_table (id,name,age)\nSELECT cast(id as INT) as id, name, cast (age as INT) as age\nFROM s3(\n    "uri" = "http://127.0.0.1:9312/test2/test.snappy.parquet",\n    "s3.access_key"= "ak",\n    "s3.secret_key" = "sk",\n    "format" = "parquet",\n    "use_path_style" = "true");\n'})}),"\n",(0,i.jsx)(n.h3,{id:"note",children:"Note"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["If the URI specified by the ",(0,i.jsx)(n.code,{children:"S3 / HDFS"})," TVF is not matched with the file, or all the matched files are empty files, then the",(0,i.jsx)(n.code,{children:" S3 / HDFS"})," TVF will return to the empty result set. In this case, using the ",(0,i.jsx)(n.code,{children:"DESC FUNCTION"})," to view the schema of this file, you will get a dummy column",(0,i.jsx)(n.code,{children:" __dummy_col"}),", which can be ignored."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["If the format of the TVF is specified to ",(0,i.jsx)(n.code,{children:"CSV"}),", and the read file is not a empty file but the first line of this file is empty, then it will prompt the error ",(0,i.jsx)(n.code,{children:"The first line is empty, can not parse column numbers"}),". This is because the schema cannot be parsed from the first line of the file"]}),"\n"]}),"\n"]})]})}function h(e={}){let{wrapper:n}={...(0,l.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(o,{...e})}):o(e)}},250065:function(e,n,s){s.d(n,{Z:function(){return r},a:function(){return a}});var t=s(667294);let i={},l=t.createContext(i);function a(e){let n=t.useContext(l);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(l.Provider,{value:n},e.children)}}}]);